---
layout: layouts/blog-post.njk
title: "Instant3D: Fast Text-to-3D with Sparse-View Generation and Large Reconstruction Model"
date: 2024-02-16
author: Itzik Ben-Shabat
permalink: "/blog/posts/2024-02-16-instant3d.html"
---

<div class="post-content">


<div id="buzzsprout-player-"></div><script charset="utf-8" src="https://www.buzzsprout.com/1914034/14515265.js?container_id=buzzsprout-player-&amp;player=small" type="text/javascript"></script>
<p>In the latest episode of the Talking Papers Podcast, I had the pleasure of hosting Jiahao Li, a talented PhD student at Toyota Technological Institute at Chicago (TTIC). Our conversation centered around his paper titled â€œInstant3D: Fast Text-to-3D with Sparse-View Generation and Large Reconstruction Model,â€ which was recently published in ICLR 2024.</p>
<p>The paper addresses the challenges of text-to-3D generation, specifically the slow inference, low diversity, and quality issues faced by existing methods. Jiahao introduced Instant3D, a novel approach that generates high-quality and diverse 3D assets from text prompts in a feed-forward manner. This is achieved through a two-stage paradigm involving the generation of a sparse set of four structured and consistent views from text, followed by the direct regression of the NeRF (Neural Radiance Field) from the generated images using a transformer-based sparse-view reconstructor.</p>
<p>I must say that the results are truly impressive, especially given the remarkable speed at which they are generated. As someone with a strong inclination towards 3D, the notion of going through a 2D projection initially felt peculiar to me. However, I canâ€™t argue with the visually striking outputs that Instant3D produces. This research underscores the importance of obtaining more and better 3D data, further pushing the boundaries of text-to-3D conversion.</p>
<p>Itâ€™s worth mentioning that I was introduced to Jiahao through Yicong Hong, another guest on the podcast who coincidentally happens to be a co-author on this paper as well. Yicong was aPhD student at the Australian National University (ANU) while I was doing my postdoc there, also interned at Adobe with Jiahao. Itâ€™s always interesting to see connections and collaborations come full circle in the research community.</p>
<p>While I regret that the Instant3D model is not made public, I understand Adobeâ€™s decision, considering the substantial computational resources required to train such models and copyright issues. Nevertheless, I am excited to see what future research Jiahao and his collaborators will bring to the field of text-to-3D conversion. Stay tuned for more exciting episodes of the Talking Papers Podcast, where we continue to delve into the latest research and discoveries in academia.</p>
<h2 class="wp-block-heading">AUTHORS</h2>
<p><br/>Jiahao Li, Hao Tan, Kai Zhang, Zexiang Xu, Fujun Luan, Yinghao Xu, Yicong Hong, Kalyan Sunkavalli, Greg Shakhnarovich, Sai Bi</p>
<h2 class="wp-block-heading">ABSTRACT</h2>
<p><br/>Text-to-3D with diffusion models has achieved remarkable progress in recent years. However, existing methods either rely on score distillation-based optimization which suffer from slow inference, low diversity and Janus problems, or are feed-forward methods that generate low-quality results due to the scarcity of 3D training data. In this paper, we propose Instant3D, a novel method that generates high-quality and diverse 3D assets from text prompts in a feed-forward manner. We adopt a two-stage paradigm, which first generates a sparse set of four structured and consistent views from text in one shot with a fine-tuned 2D text-to-image diffusion model, and then directly regresses the NeRF from the generated images with a novel transformer-based sparse-view reconstructor. Through extensive experiments, we demonstrate that our method can generate diverse 3D assets of high visual quality within 20 seconds, which is two orders of magnitude faster than previous optimization-based methods that can take 1 to 10 hours. Our project webpage: this https URL.</p>
<h2 class="wp-block-heading">RELATED (WORKS|PAPERS)</h2>
<p>ğŸ“š<a href="https://dreamfusion3d.github.io/" rel="noreferrer noopener" target="_blank">DreamFusion</a></p>
<p>ğŸ“š<a href="https://github.com/openai/shap-e" rel="noreferrer noopener" target="_blank">Shape-E</a></p>
<p>ğŸ“š<a href="https://ml.cs.tsinghua.edu.cn/prolificdreamer/" rel="noreferrer noopener" target="_blank">Prolific Dreamer</a></p>
<h2 class="wp-block-heading">LINKS AND RESOURCES</h2>
<p>ğŸ“š<a href="https://arxiv.org/abs/2311.06214" rel="noreferrer noopener" target="_blank">Preprint</a></p>
<p>ğŸ’»<a href="https://jiahao.ai/instant3d/" rel="noreferrer noopener" target="_blank">Project page</a></p>
<p>To stay up to date with his latest research, follow on:</p>
<p>ğŸ‘¨ğŸ»â€ğŸ“<a href="https://jiahao.ai/" rel="noreferrer noopener" target="_blank">Personal website</a></p>
<p>ğŸ‘¨ğŸ»â€ğŸ“<a href="https://scholar.google.com/citations?user=w9jtLkIAAAAJ" rel="noreferrer noopener" target="_blank">Google scholar</a></p>
<p>ğŸ¦<a href="https://twitter.com/JiahaoLi95" rel="noreferrer noopener" target="_blank">Twitter</a></p>
<p>ğŸ‘¨ğŸ»â€ğŸ“<a href="https://www.linkedin.com/in/jiahaoli95/" rel="noreferrer noopener" target="_blank">LinkedIn</a></p>
<figure class="wp-block-embed is-type-rich is-provider-embed-handler wp-block-embed-embed-handler"><div class="wp-block-embed__wrapper">
<!--YouTube Error: bad URL entered-->
</div></figure>
<figure class="wp-block-embed is-type-rich is-provider-embed-handler wp-block-embed-embed-handler wp-embed-aspect-16-9 wp-has-aspect-ratio"><div class="wp-block-embed__wrapper">
<iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="" frameborder="0" height="450" referrerpolicy="strict-origin-when-cross-origin" src="https://www.youtube.com/embed/8Wr4wsSPM20?feature=oembed" title="Instant3D (ICLR, 2024) with Jiahao Li on Talking Papers Podcast" width="800"></iframe>
</div></figure>
<p>This episode was recorded on January 24th 2024</p>
<h2 class="wp-block-heading">CONTACT</h2>
<p><br/>If you would like to be a guest, sponsor or share your thoughts, feel free to reach out via email: <a class="__cf_email__" data-cfemail="aedacfc2c5c7c0c980decfdecbdcdd80dec1cacdcfdddaeec9c3cfc7c280cdc1c3" href="/cdn-cgi/l/email-protection">[emailÂ protected]</a></p>
<h2 class="wp-block-heading">SUBSCRIBE AND FOLLOW</h2>
<p><br/>ğŸ§Subscribe on your favourite <a href="https://talking.papers.podcast.itzikbs.com" rel="noreferrer noopener" target="_blank">podcast app</a></p>
<p>ğŸ“§Subscribe to our <a href="http://eepurl.com/hRznqb" rel="noreferrer noopener" target="_blank">mailing list</a></p>
<p>ğŸ¦Follow us on <a href="https://twitter.com/talking_papers" rel="noreferrer noopener" target="_blank">Twitter</a></p>
<p>ğŸ¥Subscribe to our </p>
 

</div>