---
layout: layouts/blog-post.njk
title: "VLN BERT:  A Recurrent Vision-and-Language BERT for Navigation"
date: 2022-02-24
author: Itzik Ben-Shabat
permalink: "/blog/posts/2022-02-24-vln-bert-a-recurrent-vision-and-language-bert-for-navigation.html"
---

<div class="post-content">


<p>In this episode of theÂ <a href="https://www.itzikbs.com/the-talking-papers-podcast" rel="noreferrer noopener" target="_blank">Talking Papers Podcast</a>, I hosted<a href="http://www.yiconghong.me/" rel="noreferrer noopener" target="_blank"> Yicong Hong</a> to chat about his paper â€œVLN BERT: A Recurrent Vision-and-Language BERT for Navigationâ€, published in CVPR 2021. In this paper, they take on the task of vision and language navigation (VLN) and propose a time-aware recurrent BERT model. The recurrent function maintains the cross-modal state information for the agent, enabling them to achieve state-of-the-art results. When I started my postdoc position at ANU, Yicong was in the first year of his PhD. Since then, it was a delight to see him grow as a researcher. One of the things I love most about his style is his relentlessness, he wonâ€™t let it go until he figures it out (reminds me of someoneâ€¦). Yicong is a great early career researcher (soon to complete his PhD) and it was a pleasure recording this episode with him.</p>
<div id="buzzsprout-player-10091519">Â </div>
<p><script charset="utf-8" src="https://www.buzzsprout.com/1914034/10091519-yicong-hong-vln-bert.js?container_id=buzzsprout-player-10091519&amp;player=small" type="text/javascript"></script></p>
<h2 class="wp-block-heading" id="authors">AUTHORS</h2>
<p id="stephen-gould-richard-hartleydylan-campbell">Yicong Hong, Qi Wu, Yuankai Qi, Cristian Rodriguez-Opazo, Stephen Gould</p>
<p>Â </p>
<h2 class="wp-block-heading" id="abstract">ABSTRACT</h2>
<p>Accuracy of many visiolinguistic tasks has benefited significantly from the application of vision-and-language (V&amp;L) BERT. However, its application for the task of vision-and-language navigation (VLN) remains limited. One reason for this is the difficulty adapting the BERT architecture to the partially observable Markov decision process present in VLN, requiring history-dependent attention and decision making. In this paper, we propose a recurrent BERT model that is time-aware for use in VLN. Specifically, we equip the BERT model with a recurrent function that maintains cross-modal state information for the agent. Through extensive experiments on R2R and REVERIE we demonstrate that our model can replace more complex encoder-decoder models to achieve state-of-the-art results. Moreover, our approach can be generalised to other transformer-based architectures, supports pre-training, and is capable of solving navigation and referring expression tasks simultaneously.</p>
<p>Â </p>
<h2 class="wp-block-heading" id="related-papers">RELATED (WORKS|PAPERS)</h2>
<p>ğŸ“š <a href="https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf" rel="noreferrer noopener" target="_blank">Attention is All You Need</a></p>
<p>ğŸ“š <a href="https://openaccess.thecvf.com/content_CVPR_2020/html/Hao_Towards_Learning_a_Generic_Agent_for_Vision-and-Language_Navigation_via_Pre-Training_CVPR_2020_paper.html" rel="noreferrer noopener" target="_blank">Towards learning a generic agent for vision-and-language navigation via pre-training</a></p>
<h2 class="wp-block-heading" id="links-and-resources">LINKS AND RESOURCES</h2>
<p>ğŸ’» Project Page and CODE: <a href="https://github.com/YicongHong/Recurrent-VLN-BERT" rel="noreferrer noopener" target="_blank">https://github.com/YicongHong/Recurrent-VLN-BERT</a></p>
<p>ğŸ“š <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Hong_VLN_BERT_A_Recurrent_Vision-and-Language_BERT_for_Navigation_CVPR_2021_paper.pdf" rel="noreferrer noopener" target="_blank">Paper</a></p>
<figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"><div class="wp-block-embed__wrapper">
<iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="" frameborder="0" height="450" referrerpolicy="strict-origin-when-cross-origin" src="https://www.youtube.com/embed/ecwurlUBAkc?feature=oembed" title="Talking Papers Podcast with Yicong Hong - VLN BERT" width="800"></iframe>
</div></figure>
<p>Â </p>
<p>This episode was recorded on April, 16th 2021.</p>
<p>Â </p>
<h2 class="wp-block-heading" id="contact">CONTACT</h2>
<p>If you would like to be a guest, sponsor or just share your thoughts, feel free to reach out via email: <a class="__cf_email__" data-cfemail="cbbfaaa7a0a2a5ace5bbaabbaeb9b8e5bba4afa8aab8bf8baca6aaa2a7e5a8a4a6" href="/cdn-cgi/l/email-protection">[emailÂ protected]</a></p>
<h2 class="wp-block-heading" id="subscribe-and-follow"><br/>SUBSCRIBE AND FOLLOW</h2>
<p>ğŸ§Subscribe on your favourite podcast app: <a href="https://talking.papers.podcast.itzikbs.com" rel="noreferrer noopener" target="_blank">https://talking.papers.podcast.itzikbs.com</a></p>
<p>ğŸ“§Subscribe to our mailing list: <a href="http://eepurl.com/hRznqb" rel="noreferrer noopener" target="_blank">http://eepurl.com/hRznqb</a></p>
<p>ğŸ¦Follow us on Twitter: <a href="https://twitter.com/talking_papers" rel="noreferrer noopener" target="_blank">https://twitter.com/talking_papers</a></p>
<p>ğŸ¥YouTube Channel: </p>
 

</div>