---
layout: layouts/blog-post.njk
title: "HMD-NeMo: Online 3D Avatar Motion Generation From Sparse Observations"
date: 2023-11-01
author: Itzik Ben-Shabat
permalink: "/blog/posts/2023-11-01-hmdnemo.html"
---

<div class="post-content">
{% raw %}




<p>Welcome back to the Talking Papers Podcast! In our latest episode, we had the privilege of hosting the brilliant Sadegh Aliakbarian to delve into his groundbreaking ICCV 2023 paper, â€œHMD-NeMo: Online 3D Avatar Motion Generation From Sparse Observationsâ€ . Sadegh will take us on a journey through this pivotal research that addresses a crucial aspect of immersive mixed reality experiences.</p>
<p>The quality of these experiences hinges on generating plausible and precise full-body avatar motion, a challenge given the limited input signals provided by Head-Mounted Devices (HMDs), typically head and hands 6-DoF. While recent approaches have made strides in generating full-body motion from such inputs, they assume full hand visibility. This assumption, however, doesnâ€™t hold in scenarios without motion controllers, relying instead on egocentric hand tracking, which can lead to partial hand visibility due to the HMDâ€™s field of view.</p>
<p></p>
<p>â€œHMD-NeMoâ€ presents a groundbreaking solution, offering a unified approach to generating realistic full-body motion even when hands are only partially visible. This lightweight neural network operates in real-time, incorporating a spatio-temporal encoder with adaptable mask tokens, ensuring plausible motion in the absence of complete hand observations.</p>
<p></p>
<p>Sadegh is currently a senior research scientist at Microsoft Mixed Reality and AI Lab-Cambridge (UK), where heâ€™s at the forefront of Microsoft Mesh and avatar motion generation. He holds a PhD from the Australian National University, where he specialized in generative modeling of human motion. His research journey includes internships at Amazon AI, Five AI, and Qualcomm AI Research, focusing on generative models, representation learning, and adversarial examples.</p>
<p></p>
<p> We first crossed paths during our time at the Australian Centre for Robotic Vision (ACRV), where Sadegh was pursuing his PhD, and I was embarking on my postdoctoral journey. During this time, I had the privilege of collaborating with another co-author of the paper, Fatemeh Saleh, who also happens to be Sadeghâ€™s life partner. Itâ€™s been incredible to witness their continued growth.</p>
<p>ğŸš€ Join us as we uncover the critical advancements brought by â€œHMD-NeMoâ€ and their implications for the future of mixed reality experiences. </p>
<div id="buzzsprout-player-13678950">{% endraw %}
</div><script charset="utf-8" src="https://www.buzzsprout.com/1914034/13678950-hmd-nemo-sadegh-aliakbarian.js?container_id=buzzsprout-player-13678950&amp;player=small" type="text/javascript"></script>
<h2 class="wp-block-heading" id="authors">AUTHORS</h2>
<p>Sherwin Bahmani, Jeong Joon Park, Despoina Paschalidou, Xingguang Yan, Gordon Wetzstein, Leonidas Sadegh Aliakbarian, Fatemeh Saleh, David Collier, Pashmina Cameron, Darren Cosker<br/></p>
<h2 class="wp-block-heading" id="abstract">ABSTRACT</h2>
<p>Generating both plausible and accurate full body avatar motion is the key to the quality of immersive experiences in mixed reality scenarios. Head-Mounted Devices (HMDs) typically only provide a few input signals, such as head and hands 6-DoF. Recently, different approaches achieved impressive performance in generating full body motion given only head and hands signal. However, to the best of our knowledge, all existing approaches rely on full hand visibility. While this is the case when, e.g., using motion controllers, a considerable proportion of mixed reality experiences do not involve motion controllers and instead rely on egocentric hand tracking. This introduces the challenge of partial hand visibility owing to the restricted field of view of the HMD. In this paper, we propose the first unified approach, HMD-NeMo, that addresses plausible and accurate full body motion generation even when the hands may be only partially visible. HMD-NeMo is a lightweight neural network that predicts the full body motion in an online and real-time fashion. At the heart of HMD-NeMo is the spatio-temporal encoder with novel temporally adaptable mask tokens that encourage plausible motion in the absence of hand observations. We perform extensive analysis of the impact of different components in HMD-NeMo and introduce a new state-of-the-art on AMASS dataset through our evaluation.</p>
<p>Â </p>
<p></p>
<h2 class="wp-block-heading" id="related-papers">RELATED (WORKS|PAPERS)</h2>
<p>ğŸ“š<a href="https://siplab.org/projects/AvatarPoser" rel="noreferrer noopener" target="_blank">Avatar Poser</a></p>
<p></p>
<h2 class="wp-block-heading">LINKS AND RESOURCES</h2>
<p>ğŸ“š <a href="https://openaccess.thecvf.com/content/ICCV2023/html/Aliakbarian_HMD-NeMo_Online_3D_Avatar_Motion_Generation_From_Sparse_Observations_ICCV_2023_paper.html" rel="noreferrer noopener" target="_blank">Paper</a></p>
<p></p>
<p></p>
<p></p>
<p>To stay up to date with their latest research, follow on:</p>
<p>ğŸ‘¨ğŸ»â€ğŸ“<a href="https://sadegh-aa.github.io/">Personal page</a></p>
<p>ğŸ¦<a href="https://twitter.com/aa_sadegh">Twitter</a></p>
<p>ğŸ‘¨ğŸ»â€ğŸ“<a href="https://scholar.google.com/citations?user=1qXJQ7cAAAAJ&amp;hl=en&amp;oi=ao">Google Scholar</a></p>
<p></p>
<p></p>
<figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"><div class="wp-block-embed__wrapper">
<iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="" frameborder="0" height="450" referrerpolicy="strict-origin-when-cross-origin" src="https://www.youtube.com/embed/o_zAFrcnYOM?feature=oembed" title="HMDNeMo  (ICCV 2023) with Sadegh Aliakbarian on Talking Papers Podcast" width="800"></iframe>
</div></figure>
<p>Recorded on September 22nd 2023.</p>
<p><br/><br/>CONTACT</p>
<p>If you would like to be a guest, sponsor or share your thoughts, feel free to reach out via email: <a class="__cf_email__" data-cfemail="9febfef3f4f6f1f8b1effeeffaedecb1eff0fbfcfeecebdff8f2fef6f3b1fcf0f2" href="/cdn-cgi/l/email-protection">[emailÂ protected]</a></p>
<p></p>
<h2 class="wp-block-heading">SUBSCRIBE AND FOLLOW</h2>
<p>ğŸ§Subscribe on your favourite podcast app: <a href="https://talking.papers.podcast.itzikbs.com" rel="noreferrer noopener" target="_blank">https://talking.papers.podcast.itzikbs.com</a></p>
<p>ğŸ“§Subscribe to our mailing list: <a href="http://eepurl.com/hRznqb" rel="noreferrer noopener" target="_blank">http://eepurl.com/hRznqb</a></p>
<p>ğŸ¦Follow us on Twitter: <a href="https://twitter.com/talking_papers" rel="noreferrer noopener" target="_blank">https://twitter.com/talking_papers</a></p>
<p>ğŸ¥YouTube Channel: </p>
<p></p>
<p></p>
<p></p>
 

</div>