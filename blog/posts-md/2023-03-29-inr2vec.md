---
layout: layouts/blog-post.njk
title: "INR2Vec: Deep Learning on Implicit Neural Representations of Shapes"
date: 2023-03-29
author: Itzik Ben-Shabat
permalink: "/blog/posts/2023-03-29-inr2vec.html"
---

<div class="post-content">


<p>In this episode of theÂ <a href="https://www.itzikbs.com/the-talking-papers-podcast" rel="noreferrer noopener" target="_blank">Talking Papers Podcast</a>, I hosted <em>Luca De Luigi</em>. We had a great chat about his paper â€œ<a href="https://arxiv.org/abs/2302.05438" rel="noreferrer noopener" target="_blank">Deep Learning on Implicit Neural Representations of Shapes</a>â€, AKA INR2Vec published in ICLR 2023.</p>
<p>In this paper, they take implicit neural representations to the next level and use them as input signals for neural networks to solve multiple downstream tasks. The core idea was captured by one of the authors in a very catchy and concise <a href="https://twitter.com/LuigiDi77998956/status/1626575649434640384" rel="noreferrer noopener" target="_blank">tweet</a>: â€œSignals are networks so networks are data and so networks can process other networks to understand and generate signalsâ€. </p>
<p></p>
<p>Luca recently received his PhD from the University of Bolognia and is currently working at a startup based in Bolognia <a href="https://www.eyecan.ai/" rel="noreferrer noopener" target="_blank">eyecan.ai</a>. His research focus is on neural representations of signals, especially for 3D geometry. To be honest, I knew I wanted to get Luca on the podcast the second I saw the paper on arXiv because I was working on a related topic but had to shelf it due to time management issues. This paper got me excited about that topic again. I didnâ€™t know Luca before recording the episode and it was a delight to get to know him and his work.  </p>
<p></p>
<p></p>
<div id="buzzsprout-player-12544865"></div><script charset="utf-8" src="https://www.buzzsprout.com/1914034/12544865-inr2vec-luca-de-luigi.js?container_id=buzzsprout-player-12544865&amp;player=small" type="text/javascript"></script>
<h2 class="wp-block-heading" id="authors">AUTHORS</h2>
<p id="stephen-gould-richard-hartleydylan-campbell"><em>Luca De Luigi,Â Adriano Cardace,Â Riccardo Spezialetti,Â Pierluigi Zama Ramirez,Â Samuele Salti,Â Luigi Di Stefano</em></p>
<p><br/></p>
<h2 class="wp-block-heading" id="abstract">ABSTRACT</h2>
<p>Â </p>
<p>Abstraction is at the heart of sketching due to the simple and minimal nature of line drawings. Abstraction entails identifying the essential visual properties of an object or scene, which requires Implicit Neural Representations (INRs) have emerged in the last few years as a powerful tool to encode continuously a variety of different signals like images, videos, audio and 3D shapes. When applied to 3D shapes, INRs allow to overcome the fragmentation and shortcomings of the popular discrete representations used so far. Yet, considering that INRs consist in neural networks, it is not clear whether and how it may be possible to feed them into deep learning pipelines aimed at solving a downstream task. In this paper, we put forward this research problem and propose inr2vec, a framework that can compute a compact latent representation for an input INR in a single inference pass. We verify that inr2vec can embed effectively the 3D shapes represented by the input INRs and show how the produced embeddings can be fed into deep learning pipelines to solve several tasks by processing exclusively INRs.</p>
<p>Â </p>
<p></p>
<h2 class="wp-block-heading" id="related-papers">RELATED (WORKS|PAPERS)</h2>
<p>ğŸ“š</p>
<p>ğŸ“š</p>
<p>ğŸ“š<a href="https://arxiv.org/abs/1612.00593" rel="noreferrer noopener" target="_blank">PointNet</a></p>
<p></p>
<h2 class="wp-block-heading">LINKS AND RESOURCES</h2>
<p>ğŸ“š <a href="https://arxiv.org/abs/2302.05438" rel="noreferrer noopener" target="_blank">Paper</a></p>
<p>ğŸ’»<a href="https://cvlab-unibo.github.io/inr2vec/" rel="noreferrer noopener" target="_blank">Project page</a></p>
<p></p>
<p></p>
<p>To stay up to date with Lucaâ€™s latest research, follow her on:</p>
<p>ğŸ‘¨ğŸ»â€ğŸ“<a href="https://scholar.google.com/citations?user=PpHLOpQAAAAJ&amp;hl=it" rel="noreferrer noopener" target="_blank">Google Scholar</a></p>
<p>ğŸ‘¨ğŸ»â€ğŸ“<a href="https://www.linkedin.com/in/luca-de-luigi-4a713bba/?originalSubdomain=it" rel="noreferrer noopener" target="_blank">LinkedIn</a></p>
<p></p>
<p></p>
<figure class="wp-block-embed is-type-rich is-provider-embed-handler wp-block-embed-embed-handler wp-embed-aspect-16-9 wp-has-aspect-ratio"><div class="wp-block-embed__wrapper">
<iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="" frameborder="0" height="450" referrerpolicy="strict-origin-when-cross-origin" src="https://www.youtube.com/embed/RZejWGuGiAw?feature=oembed" title="INR2Vec (ICLR 2023) with Luca De Luigi on Talking papers" width="800"></iframe>
</div></figure>
<p>Recorded on March 22 2023.</p>
<h3 class="wp-block-heading">SPONSOR</h3>
<p>This episode was sponsored by YOOM. YOOM is an Israeli startup dedicated to volumetric video creation. They were voted as the 2022 best start-up to work for by Dunâ€™s 100.<br/><a href="https://www.yoom.com/careers" rel="noreferrer noopener" target="_blank">Join their team</a> that works on geometric deep learning research, implicit representations of 3D humans, NeRFs, and 3D/4D generative models.</p>
<p><br/>Visit <a href="https://www.yoom.com/" rel="noreferrer noopener" target="_blank">YOOM.com</a>.</p>
<h2 class="wp-block-heading"><br/>CONTACT</h2>
<p>If you would like to be a guest, sponsor or share your thoughts, feel free to reach out via email: <a class="__cf_email__" data-cfemail="097d68656260676e277968796c7b7a2779666d6a687a7d496e64686065276a6664" href="/cdn-cgi/l/email-protection">[emailÂ protected]</a></p>
<p></p>
<h2 class="wp-block-heading">SUBSCRIBE AND FOLLOW</h2>
<p>ğŸ§Subscribe on your favourite podcast app: <a href="https://talking.papers.podcast.itzikbs.com" rel="noreferrer noopener" target="_blank">https://talking.papers.podcast.itzikbs.com</a></p>
<p>ğŸ“§Subscribe to our mailing list: <a href="http://eepurl.com/hRznqb" rel="noreferrer noopener" target="_blank">http://eepurl.com/hRznqb</a></p>
<p>ğŸ¦Follow us on Twitter: <a href="https://twitter.com/talking_papers" rel="noreferrer noopener" target="_blank">https://twitter.com/talking_papers</a></p>
<p>ğŸ¥YouTube Channel: </p>
<p></p>
<p></p>
<p></p>
 

</div>