---
layout: layouts/blog-post.njk
title: "CVPR 2019"
date: 2019-06-25
author: Itzik Ben-Shabat
permalink: "/blog/posts/2019-06-25-cvpr-2019.html"
---

<div class="post-content">


<p>About three months ago, I hit one of my first career goals – have a paper accepted to CVPR (achievement unlocked ! ). Last week I  had the privilege to present a poster of our work on <a aria-label="normal estimation for unstructured 3D point clouds (opens in a new tab)" href="https://www.itzikbs.com/nesti-net-normal-estimation-for-3d-point-clouds" rel="noreferrer noopener" target="_blank">normal estimation for unstructured 3D point clouds using CNNs</a> at CVPR 2019 in Long Beach California (16-20.6.19). <br/><br/>In this post, I will summarize my experience from the conference and highlight some papers and workshops that I found interesting (mostly 3D point cloud-related topics but also some nice applications, ideas, and datasets).<br/><br/>If you missed it, the oral presentations are available on this <a aria-label="YouTube playlis (opens in a new tab)" href="https://www.youtube.com/playlist?list=PLzGR8_vFq07KBRXNT8SmuhBUp1tHRsglm" rel="noreferrer noopener" target="_blank">YouTube playlist</a> I composed. All of the computer vision foundation videos are available <a aria-label="here (opens in a new tab)" href="https://www.youtube.com/channel/UC0n76gicaarsN_Y9YShWwhw/videos" rel="noreferrer noopener" target="_blank">here</a>. </p>
<figure class="wp-block-image"><picture>
<source media="(min-width: 800px)" srcset="../../assets/images/blog/IMG_20190620_082711-800.webp" type="image/webp"/>
<source media="(max-width: 799px)" srcset="../../assets/images/blog/IMG_20190620_082711-400.webp" type="image/webp"/>
<source media="(max-width: 399px)" srcset="../../assets/images/blog/IMG_20190620_082711-200.webp" type="image/webp"/>
<img alt="" class="wp-image-1024" height="551" loading="lazy" src="../../assets/images/blog/IMG_20190620_082711.jpg" width="1024"/>
</picture>
</figure>
<h2 class="wp-block-heading">Statistics</h2>
<p>Get ready for some numbers that will blow you away… <br/>This year CVPR hosted ~9200 attendees from 68 countries. 134 of them are from my home country of Israel (an amazing number considering Israel’s size of the population, as a reference, Germany had 265). In addition, 14104 Authors submitted 5160 papers, of which 1294 were accepted.  If this exponential growth will continue they are expecting 10.8 Billion papers by 2028 (not really).  <br/> For all the detail see the <a aria-label="IEEE tech news post. (opens in a new tab)" href="https://www.computer.org/publications/tech-news/events/ieee-cvpr-conference-on-computer-vision-and-pattern-recognition-2019-awards-records" rel="noreferrer noopener" target="_blank">IEEE tech news post.</a> </p>
<h2 class="wp-block-heading">Awards</h2>
<p>I think the most interesting award is the  PAMI Longuet-Higgins Prize  “retrospective most impactful paper from CVPR 2009” which was awarded to a paper that achieved the “test of time”. Not very surprisingly, it was given to: <br/><br/> “ImageNet: A large-scale hierarchical image database” by Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. <br/>Very well deserved. When you think about it, it started a new genre of immensely large dataset papers. </p>
<p>Some Israeli pride –  best Paper, Honorable Mention: Zhengqi Li, Tali Dekel, Forrester Cole, Richard Tucker, Ce Liu, Bill Freeman, and Noah Snavely for their “Learning the Depths of Moving People by Watching Frozen People.”  paper. (The paper is not from Israel, but Tali Dekel is… or was… Rumor says she is coming back soon… )<br/> <br/>Best Paper: Shumian Xin, Sotiris Nousias, Kyros Kutulakos, Aswin Sankaranarayanan, Srinivasa G. Narasimhan, and Ioannis Gkioulekas for their “A Theory of Fermat Paths for Non-Line-of-Sight Shape Reconstruction.” From Carnegie Mellon University, University of Toronto, and University College London. </p>
<h2 class="wp-block-heading">Workshops</h2>
<p>In the first two days I attended some great workshops that hosted top tier speakers: </p>
<ul class="wp-block-list"><li><a aria-label="3D Scene Generation (opens in a new tab)" href="https://3dscenegen.github.io/" rel="noreferrer noopener" target="_blank">3D Scene Generation</a></li></ul>
<p> It included one of my favorite spotlight posters of a Eurographics honorable mention paper:   <a aria-label=" (opens in a new tab)" href="http://ai.stanford.edu/~hewang/" rel="noreferrer noopener" target="_blank">Learning a Generative Model for Multi-Step Human-Object Interactions from Videos.</a> He Wang<strong>*</strong>, Soeren Pirk*, Ersin Yumer, Vladimir Kim, Ozan Sener, Srinath Sridhar, Leonidas J. Guibas.<br/>A talk by Vladlan Koltun provided some interesting empirical evidence to what single-view reconstruction networks learn. Clearly stating that they perform retrieval rather than reconstruction and presented their Oracle NN to support this claim. </p>
<ul class="wp-block-list"><li><a href="https://futurecv.github.io/" rel="noreferrer noopener" target="_blank">Computer Vision After 5 Years</a></li></ul>
<p>Probably the best workshop at the conference. It was so good that I wasn’t able to attend since the room was so full there wasn’t even room to stand. I heard that Prof. Bill Freeman told the computer vision story using rock songs.  </p>
<ul class="wp-block-list"><li><a aria-label="Tutorial on Casul Networks (opens in a new tab)" href="https://www.crcv.ucf.edu/cvpr2019-tutorial/" rel="noreferrer noopener" target="_blank">Tutorial on Casule Networks</a></li></ul>
<p>Capsule networks are getting increasing attention lately so I was curious what the fuss is all about. After the tutorial, I heard some discussions questioning the effectiveness of Capsule Networks. Geoffrey Hinton said in one of his recent interviews that this is the most interesting topic he is working on today. He usually gets it right… only time will tell.  It was nice to see that it is also already extended to point cloud processing.</p>
<ul class="wp-block-list"><li><a aria-label="3D Scene Understanding for Vision, Graphics, and Robotics (opens in a new tab)" href="https://scene-understanding.com/" rel="noreferrer noopener" target="_blank">3D Scene Understanding for Vision, Graphics, and Robotics</a> – </li></ul>
<p>I especially enjoyed Prof. Thomas Funkhouser’s (Princeton) talk about their <a href="https://www.youtube.com/watch?v=-O-E1nFm6-A" rel="noreferrer noopener" target="_blank">TossingBot</a> paper and the idea of “Residual Physics”.</p>
<h2 class="wp-block-heading">Favorite Posters</h2>
<h4 class="wp-block-heading"> Day1</h4>
<ol class="wp-block-list"><li><a href="http://openaccess.thecvf.com/content_CVPR_2019/html/Duan_Structural_Relational_Reasoning_of_Point_Clouds_CVPR_2019_paper.html" rel="noreferrer noopener" target="_blank">Structural Relational Reasoning of Point Clouds</a><a href="http://openaccess.thecvf.com/CVPR2019.py#" rel="noreferrer noopener" target="_blank">Yueqi Duan</a>, <a href="http://openaccess.thecvf.com/CVPR2019.py#" rel="noreferrer noopener" target="_blank">Yu Zheng</a>, <a href="http://openaccess.thecvf.com/CVPR2019.py#" rel="noreferrer noopener" target="_blank">Jiwen Lu</a>, <a href="http://openaccess.thecvf.com/CVPR2019.py#" rel="noreferrer noopener" target="_blank">Jie Zhou</a>, <a href="http://openaccess.thecvf.com/CVPR2019.py#" rel="noreferrer noopener" target="_blank">Qi Tian</a>[<a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Duan_Structural_Relational_Reasoning_of_Point_Clouds_CVPR_2019_paper.pdf" rel="noreferrer noopener" target="_blank">pdf</a>] [<a>bibtex</a>]</li><li><a href="http://openaccess.thecvf.com/content_CVPR_2019/html/Zhao_3D_Point_Capsule_Networks_CVPR_2019_paper.html" rel="noreferrer noopener" target="_blank">3D Point Capsule Networks</a> <br/><a href="http://openaccess.thecvf.com/CVPR2019.py#" rel="noreferrer noopener" target="_blank">Yongheng Zhao</a>, <a href="http://openaccess.thecvf.com/CVPR2019.py#" rel="noreferrer noopener" target="_blank">Tolga Birdal</a>, <a href="http://openaccess.thecvf.com/CVPR2019.py#" rel="noreferrer noopener" target="_blank">Haowen Deng</a>, <a href="http://openaccess.thecvf.com/CVPR2019.py#" rel="noreferrer noopener" target="_blank">Federico Tombari</a><br/>[<a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Zhao_3D_Point_Capsule_Networks_CVPR_2019_paper.pdf" rel="noreferrer noopener" target="_blank">pdf</a>] [<a href="http://openaccess.thecvf.com/content_CVPR_2019/supplemental/Zhao_3D_Point_Capsule_CVPR_2019_supplemental.pdf" rel="noreferrer noopener" target="_blank">supp</a>] [<a>bibtex</a>]</li><li><a href="http://openaccess.thecvf.com/content_CVPR_2019/html/Park_DeepSDF_Learning_Continuous_Signed_Distance_Functions_for_Shape_Representation_CVPR_2019_paper.html" rel="noreferrer noopener" target="_blank">DeepSDF: Learning Continuous Signed Distance Functions for Shape Representation</a><br/><a href="http://openaccess.thecvf.com/CVPR2019.py#" rel="noreferrer noopener" target="_blank">Jeong Joon Park</a>, <a href="http://openaccess.thecvf.com/CVPR2019.py#" rel="noreferrer noopener" target="_blank">Peter Florence</a>, <a href="http://openaccess.thecvf.com/CVPR2019.py#" rel="noreferrer noopener" target="_blank">Julian Straub</a>, <a href="http://openaccess.thecvf.com/CVPR2019.py#" rel="noreferrer noopener" target="_blank">Richard Newcombe</a>, <a href="http://openaccess.thecvf.com/CVPR2019.py#" rel="noreferrer noopener" target="_blank">Steven Lovegrove</a><br/>[<a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Park_DeepSDF_Learning_Continuous_Signed_Distance_Functions_for_Shape_Representation_CVPR_2019_paper.pdf" rel="noreferrer noopener" target="_blank">pdf</a>] [<a href="http://openaccess.thecvf.com/content_CVPR_2019/supplemental/Park_DeepSDF_Learning_Continuous_CVPR_2019_supplemental.pdf" rel="noreferrer noopener" target="_blank">supp</a>] [<a>bibtex</a>]</li><li><a href="http://openaccess.thecvf.com/content_CVPR_2019/html/Shi_PointRCNN_3D_Object_Proposal_Generation_and_Detection_From_Point_Cloud_CVPR_2019_paper.html" rel="noreferrer noopener" target="_blank">PointRCNN: 3D Object Proposal Generation and Detection From Point Cloud</a><br/><a href="http://openaccess.thecvf.com/CVPR2019.py#" rel="noreferrer noopener" target="_blank">Shaoshuai Shi</a>, <a href="http://openaccess.thecvf.com/CVPR2019.py#" rel="noreferrer noopener" target="_blank">Xiaogang Wang</a>, <a href="http://openaccess.thecvf.com/CVPR2019.py#" rel="noreferrer noopener" target="_blank">Hongsheng Li</a><br/>[<a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Shi_PointRCNN_3D_Object_Proposal_Generation_and_Detection_From_Point_Cloud_CVPR_2019_paper.pdf" rel="noreferrer noopener" target="_blank">pdf</a>] [<a>bibtex</a>]</li><li><a href="http://openaccess.thecvf.com/content_CVPR_2019/html/Avetisyan_Scan2CAD_Learning_CAD_Model_Alignment_in_RGB-D_Scans_CVPR_2019_paper.html" rel="noreferrer noopener" target="_blank">Scan2CAD: Learning CAD Model Alignment in RGB-D Scans</a><br/><a href="http://openaccess.thecvf.com/CVPR2019.py#" rel="noreferrer noopener" target="_blank">Armen Avetisyan</a>, <a href="http://openaccess.thecvf.com/CVPR2019.py#" rel="noreferrer noopener" target="_blank">Manuel Dahnert</a>, <a href="http://openaccess.thecvf.com/CVPR2019.py#" rel="noreferrer noopener" target="_blank">Angela Dai</a>, <a href="http://openaccess.thecvf.com/CVPR2019.py#" rel="noreferrer noopener" target="_blank">Manolis Savva</a>, <a href="http://openaccess.thecvf.com/CVPR2019.py#" rel="noreferrer noopener" target="_blank">Angel X. Chang</a>, <a href="http://openaccess.thecvf.com/CVPR2019.py#" rel="noreferrer noopener" target="_blank">Matthias Niessner</a><br/>[<a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Avetisyan_Scan2CAD_Learning_CAD_Model_Alignment_in_RGB-D_Scans_CVPR_2019_paper.pdf" rel="noreferrer noopener" target="_blank">pdf</a>] [<a href="http://openaccess.thecvf.com/content_CVPR_2019/supplemental/Avetisyan_Scan2CAD_Learning_CAD_CVPR_2019_supplemental.pdf" rel="noreferrer noopener" target="_blank">supp</a>] [<a>bibtex</a>]</li><li><a href="http://openaccess.thecvf.com/content_CVPR_2019/html/Dovrat_Learning_to_Sample_CVPR_2019_paper.html" rel="noreferrer noopener" target="_blank">Learning to Sample</a><br/><a href="http://openaccess.thecvf.com/CVPR2019.py#" rel="noreferrer noopener" target="_blank">Oren Dovrat</a>, <a href="http://openaccess.thecvf.com/CVPR2019.py#" rel="noreferrer noopener" target="_blank">Itai Lang</a>, <a href="http://openaccess.thecvf.com/CVPR2019.py#" rel="noreferrer noopener" target="_blank">Shai Avidan</a><br/>[<a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Dovrat_Learning_to_Sample_CVPR_2019_paper.pdf" rel="noreferrer noopener" target="_blank">pdf</a>] [<a href="http://openaccess.thecvf.com/content_CVPR_2019/supplemental/Dovrat_Learning_to_Sample_CVPR_2019_supplemental.pdf" rel="noreferrer noopener" target="_blank">supp</a>] [<a>bibtex</a>]</li><li><a href="http://openaccess.thecvf.com/content_CVPR_2019/html/Muralikrishnan_Shape_Unicode_A_Unified_Shape_Representation_CVPR_2019_paper.html" rel="noreferrer noopener" target="_blank">Shape Unicode: A Unified Shape Representation</a><br/><a href="http://openaccess.thecvf.com/CVPR2019.py#" rel="noreferrer noopener" target="_blank">Sanjeev Muralikrishnan</a>, <a href="http://openaccess.thecvf.com/CVPR2019.py#" rel="noreferrer noopener" target="_blank">Vladimir G. Kim</a>, <a href="http://openaccess.thecvf.com/CVPR2019.py#" rel="noreferrer noopener" target="_blank">Matthew Fisher</a>, <a href="http://openaccess.thecvf.com/CVPR2019.py#" rel="noreferrer noopener" target="_blank">Siddhartha Chaudhuri</a><br/>[<a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Muralikrishnan_Shape_Unicode_A_Unified_Shape_Representation_CVPR_2019_paper.pdf" rel="noreferrer noopener" target="_blank">pdf</a>] [<a href="http://openaccess.thecvf.com/content_CVPR_2019/supplemental/Muralikrishnan_Shape_Unicode_A_CVPR_2019_supplemental.pdf" rel="noreferrer noopener" target="_blank">supp</a>] [<a>bibtex</a>]</li></ol>
<p>The last two  (6,7)  get an “honorable mention” from me for that day. The day ended with a great party by IntelAI.</p>
<ul class="wp-block-gallery columns-3 is-cropped wp-block-gallery-1 is-layout-flex wp-block-gallery-is-layout-flex" data-carousel-extra='{"blog_id":1,"permalink":"https:\/\/www.itzikbs.com\/cvpr-2019"}'><li class="blocks-gallery-item"><figure><picture>
<source media="(min-width: 400px)" srcset="../../assets/images/blog/IMG_20190618_153427-1024x768-400.webp" type="image/webp"/>
<source media="(max-width: 399px)" srcset="../../assets/images/blog/IMG_20190618_153427-1024x768-200.webp" type="image/webp"/>
<img alt="" class="wp-image-1030" height="600" loading="lazy" src="../../assets/images/blog/IMG_20190618_153427-1024x768.jpg" width="800"/>
</picture>
</figure></li><li class="blocks-gallery-item"><figure><picture>
<source media="(min-width: 400px)" srcset="../../assets/images/blog/IMG_20190618_153318-1024x768-400.webp" type="image/webp"/>
<source media="(max-width: 399px)" srcset="../../assets/images/blog/IMG_20190618_153318-1024x768-200.webp" type="image/webp"/>
<img alt="" class="wp-image-1029" height="600" loading="lazy" src="../../assets/images/blog/IMG_20190618_153318-1024x768.jpg" width="800"/>
</picture>
</figure></li><li class="blocks-gallery-item"><figure><picture>
<source media="(min-width: 400px)" srcset="../../assets/images/blog/IMG_20190618_153011-1024x768-400.webp" type="image/webp"/>
<source media="(max-width: 399px)" srcset="../../assets/images/blog/IMG_20190618_153011-1024x768-200.webp" type="image/webp"/>
<img alt="" class="wp-image-1028" height="600" loading="lazy" src="../../assets/images/blog/IMG_20190618_153011-1024x768.jpg" width="800"/>
</picture>
</figure></li><li class="blocks-gallery-item"><figure><picture>
<source media="(min-width: 400px)" srcset="../../assets/images/blog/IMG_20190618_150140-1024x768-400.webp" type="image/webp"/>
<source media="(max-width: 399px)" srcset="../../assets/images/blog/IMG_20190618_150140-1024x768-200.webp" type="image/webp"/>
<img alt="" class="wp-image-1027" height="600" loading="lazy" src="../../assets/images/blog/IMG_20190618_150140-1024x768.jpg" width="800"/>
</picture>
</figure></li></ul>
<h2 class="wp-block-heading">Day 2:</h2>
<ol class="wp-block-list"><li><a href="http://openaccess.thecvf.com/content_CVPR_2019/html/Li_Learning_the_Depths_of_Moving_People_by_Watching_Frozen_People_CVPR_2019_paper.html" rel="noreferrer noopener" target="_blank">Learning the Depths of Moving People by Watching Frozen People</a><br/><a href="http://openaccess.thecvf.com/CVPR2019.py#" rel="noreferrer noopener" target="_blank">Zhengqi Li</a>, <a href="http://openaccess.thecvf.com/CVPR2019.py#" rel="noreferrer noopener" target="_blank">Tali Dekel</a>, <a href="http://openaccess.thecvf.com/CVPR2019.py#" rel="noreferrer noopener" target="_blank">Forrester Cole</a>, <a href="http://openaccess.thecvf.com/CVPR2019.py#" rel="noreferrer noopener" target="_blank">Richard Tucker</a>, <a href="http://openaccess.thecvf.com/CVPR2019.py#" rel="noreferrer noopener" target="_blank">Noah Snavely</a>, <a href="http://openaccess.thecvf.com/CVPR2019.py#" rel="noreferrer noopener" target="_blank">Ce Liu</a>, <a href="http://openaccess.thecvf.com/CVPR2019.py#" rel="noreferrer noopener" target="_blank">William T. Freeman</a><br/>[<a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Li_Learning_the_Depths_of_Moving_People_by_Watching_Frozen_People_CVPR_2019_paper.pdf" rel="noreferrer noopener" target="_blank">pdf</a>] [<a href="http://openaccess.thecvf.com/content_CVPR_2019/supplemental/Li_Learning_the_Depths_CVPR_2019_supplemental.pdf" rel="noreferrer noopener" target="_blank">supp</a>] [<a>bibtex</a>]</li><li><a href="http://openaccess.thecvf.com/content_CVPR_2019/html/Mescheder_Occupancy_Networks_Learning_3D_Reconstruction_in_Function_Space_CVPR_2019_paper.html" rel="noreferrer noopener" target="_blank">Occupancy Networks: Learning 3D Reconstruction in Function Space</a><br/><a href="http://openaccess.thecvf.com/CVPR2019.py#" rel="noreferrer noopener" target="_blank">Lars Mescheder</a>, <a href="http://openaccess.thecvf.com/CVPR2019.py#" rel="noreferrer noopener" target="_blank">Michael Oechsle</a>, <a href="http://openaccess.thecvf.com/CVPR2019.py#" rel="noreferrer noopener" target="_blank">Michael Niemeyer</a>, <a href="http://openaccess.thecvf.com/CVPR2019.py#" rel="noreferrer noopener" target="_blank">Sebastian Nowozin</a>, <a href="http://openaccess.thecvf.com/CVPR2019.py#" rel="noreferrer noopener" target="_blank">Andreas Geiger</a><br/>[<a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Mescheder_Occupancy_Networks_Learning_3D_Reconstruction_in_Function_Space_CVPR_2019_paper.pdf" rel="noreferrer noopener" target="_blank">pdf</a>] [<a href="http://openaccess.thecvf.com/content_CVPR_2019/supplemental/Mescheder_Occupancy_Networks_Learning_CVPR_2019_supplemental.zip" rel="noreferrer noopener" target="_blank">supp</a>] [<a>bibtex</a>]</li><li><a href="http://openaccess.thecvf.com/content_CVPR_2019/html/Hou_3D-SIS_3D_Semantic_Instance_Segmentation_of_RGB-D_Scans_CVPR_2019_paper.html" rel="noreferrer noopener" target="_blank">3D-SIS: 3D Semantic Instance Segmentation of RGB-D Scans</a><a href="http://openaccess.thecvf.com/CVPR2019.py#" rel="noreferrer noopener" target="_blank">Ji Hou</a>, <a href="http://openaccess.thecvf.com/CVPR2019.py#" rel="noreferrer noopener" target="_blank">Angela Dai</a>, <a href="http://openaccess.thecvf.com/CVPR2019.py#" rel="noreferrer noopener" target="_blank">Matthias Niessner</a><br/>[<a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Hou_3D-SIS_3D_Semantic_Instance_Segmentation_of_RGB-D_Scans_CVPR_2019_paper.pdf" rel="noreferrer noopener" target="_blank">pdf</a>] [<a href="http://openaccess.thecvf.com/content_CVPR_2019/supplemental/Hou_3D-SIS_3D_Semantic_CVPR_2019_supplemental.pdf" rel="noreferrer noopener" target="_blank">supp</a>] [<a>bibtex</a>]</li><li><a href="http://openaccess.thecvf.com/content_CVPR_2019/html/Gojcic_The_Perfect_Match_3D_Point_Cloud_Matching_With_Smoothed_Densities_CVPR_2019_paper.html" rel="noreferrer noopener" target="_blank">The Perfect Match: 3D Point Cloud Matching With Smoothed Densities</a><br/><a href="http://openaccess.thecvf.com/CVPR2019.py#" rel="noreferrer noopener" target="_blank">Zan Gojcic</a>, <a href="http://openaccess.thecvf.com/CVPR2019.py#" rel="noreferrer noopener" target="_blank">Caifa Zhou</a>, <a href="http://openaccess.thecvf.com/CVPR2019.py#" rel="noreferrer noopener" target="_blank">Jan D. Wegner</a>, <a href="http://openaccess.thecvf.com/CVPR2019.py#" rel="noreferrer noopener" target="_blank">Andreas Wieser</a><br/>[<a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Gojcic_The_Perfect_Match_3D_Point_Cloud_Matching_With_Smoothed_Densities_CVPR_2019_paper.pdf" rel="noreferrer noopener" target="_blank">pdf</a>] [<a href="http://openaccess.thecvf.com/content_CVPR_2019/supplemental/Gojcic_The_Perfect_Match_CVPR_2019_supplemental.pdf" rel="noreferrer noopener" target="_blank">supp</a>] [<a>bibtex</a>]</li><li><a href="http://openaccess.thecvf.com/content_CVPR_2019/html/He_GeoNet_Deep_Geodesic_Networks_for_Point_Cloud_Analysis_CVPR_2019_paper.html" rel="noreferrer noopener" target="_blank">GeoNet: Deep Geodesic Networks for Point Cloud Analysis</a><br/><a href="http://openaccess.thecvf.com/CVPR2019.py#" rel="noreferrer noopener" target="_blank">Tong He</a>, <a href="http://openaccess.thecvf.com/CVPR2019.py#" rel="noreferrer noopener" target="_blank">Haibin Huang</a>, <a href="http://openaccess.thecvf.com/CVPR2019.py#" rel="noreferrer noopener" target="_blank">Li Yi</a>, <a href="http://openaccess.thecvf.com/CVPR2019.py#" rel="noreferrer noopener" target="_blank">Yuqian Zhou</a>, <a href="http://openaccess.thecvf.com/CVPR2019.py#" rel="noreferrer noopener" target="_blank">Chihao Wu</a>, <a href="http://openaccess.thecvf.com/CVPR2019.py#" rel="noreferrer noopener" target="_blank">Jue Wang</a>, <a href="http://openaccess.thecvf.com/CVPR2019.py#" rel="noreferrer noopener" target="_blank">Stefano Soatto</a><br/>[<a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/He_GeoNet_Deep_Geodesic_Networks_for_Point_Cloud_Analysis_CVPR_2019_paper.pdf" rel="noreferrer noopener" target="_blank">pdf</a>] [<a>bibtex</a>]</li><li><a href="http://openaccess.thecvf.com/content_CVPR_2019/html/Aoki_PointNetLK_Robust__Efficient_Point_Cloud_Registration_Using_PointNet_CVPR_2019_paper.html" rel="noreferrer noopener" target="_blank">PointNetLK: Robust &amp; Efficient Point Cloud Registration Using PointNet</a><br/><a href="http://openaccess.thecvf.com/CVPR2019.py#" rel="noreferrer noopener" target="_blank">Yasuhiro Aoki</a>, <a href="http://openaccess.thecvf.com/CVPR2019.py#" rel="noreferrer noopener" target="_blank">Hunter Goforth</a>, <a href="http://openaccess.thecvf.com/CVPR2019.py#" rel="noreferrer noopener" target="_blank">Rangaprasad Arun Srivatsan</a>, <a href="http://openaccess.thecvf.com/CVPR2019.py#" rel="noreferrer noopener" target="_blank">Simon Lucey</a><br/>[<a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Aoki_PointNetLK_Robust__Efficient_Point_Cloud_Registration_Using_PointNet_CVPR_2019_paper.pdf" rel="noreferrer noopener" target="_blank">pdf</a>] [<a href="http://openaccess.thecvf.com/content_CVPR_2019/supplemental/Aoki_PointNetLK_Robust__CVPR_2019_supplemental.zip" rel="noreferrer noopener" target="_blank">supp</a>] [<a>bibtex</a>]</li></ol>
<ul class="wp-block-gallery columns-3 is-cropped wp-block-gallery-2 is-layout-flex wp-block-gallery-is-layout-flex" data-carousel-extra='{"blog_id":1,"permalink":"https:\/\/www.itzikbs.com\/cvpr-2019"}'><li class="blocks-gallery-item"><figure><picture>
<source media="(min-width: 800px)" srcset="../../assets/images/blog/IMG_20190619_101010-800.webp" type="image/webp"/>
<source media="(max-width: 799px)" srcset="../../assets/images/blog/IMG_20190619_101010-400.webp" type="image/webp"/>
<source media="(max-width: 399px)" srcset="../../assets/images/blog/IMG_20190619_101010-200.webp" type="image/webp"/>
<img alt="" class="wp-image-1031" height="768" loading="lazy" src="../../assets/images/blog/IMG_20190619_101010.jpg" width="1024"/>
</picture>
</figure></li><li class="blocks-gallery-item"><figure><picture>
<source media="(min-width: 800px)" srcset="../../assets/images/blog/IMG_20190619_101045-800.webp" type="image/webp"/>
<source media="(max-width: 799px)" srcset="../../assets/images/blog/IMG_20190619_101045-400.webp" type="image/webp"/>
<source media="(max-width: 399px)" srcset="../../assets/images/blog/IMG_20190619_101045-200.webp" type="image/webp"/>
<img alt="" class="wp-image-1032" height="768" loading="lazy" src="../../assets/images/blog/IMG_20190619_101045.jpg" width="1024"/>
</picture>
</figure></li><li class="blocks-gallery-item"><figure><picture>
<source media="(min-width: 800px)" srcset="../../assets/images/blog/IMG_20190619_101801-800.webp" type="image/webp"/>
<source media="(max-width: 799px)" srcset="../../assets/images/blog/IMG_20190619_101801-400.webp" type="image/webp"/>
<source media="(max-width: 399px)" srcset="../../assets/images/blog/IMG_20190619_101801-200.webp" type="image/webp"/>
<img alt="" class="wp-image-1033" height="768" loading="lazy" src="../../assets/images/blog/IMG_20190619_101801.jpg" width="1024"/>
</picture>
</figure></li><li class="blocks-gallery-item"><figure><picture>
<source media="(min-width: 800px)" srcset="../../assets/images/blog/IMG_20190619_101847-800.webp" type="image/webp"/>
<source media="(max-width: 799px)" srcset="../../assets/images/blog/IMG_20190619_101847-400.webp" type="image/webp"/>
<source media="(max-width: 399px)" srcset="../../assets/images/blog/IMG_20190619_101847-200.webp" type="image/webp"/>
<img alt="" class="wp-image-1034" height="768" loading="lazy" src="../../assets/images/blog/IMG_20190619_101847.jpg" width="1024"/>
</picture>
</figure></li><li class="blocks-gallery-item"><figure><picture>
<source media="(min-width: 800px)" srcset="../../assets/images/blog/IMG_20190619_104519-800.webp" type="image/webp"/>
<source media="(max-width: 799px)" srcset="../../assets/images/blog/IMG_20190619_104519-400.webp" type="image/webp"/>
<source media="(max-width: 399px)" srcset="../../assets/images/blog/IMG_20190619_104519-200.webp" type="image/webp"/>
<img alt="" class="wp-image-1035" height="768" loading="lazy" src="../../assets/images/blog/IMG_20190619_104519.jpg" width="1024"/>
</picture>
</figure></li><li class="blocks-gallery-item"><figure><picture>
<source media="(min-width: 800px)" srcset="../../assets/images/blog/IMG_20190619_165544-800.webp" type="image/webp"/>
<source media="(max-width: 799px)" srcset="../../assets/images/blog/IMG_20190619_165544-400.webp" type="image/webp"/>
<source media="(max-width: 399px)" srcset="../../assets/images/blog/IMG_20190619_165544-200.webp" type="image/webp"/>
<img alt="" class="wp-image-1036" height="768" loading="lazy" src="../../assets/images/blog/IMG_20190619_165544.jpg" width="1024"/>
</picture>
</figure></li><li class="blocks-gallery-item"><figure><picture>
<source media="(min-width: 800px)" srcset="../../assets/images/blog/IMG_20190619_171157-800.webp" type="image/webp"/>
<source media="(max-width: 799px)" srcset="../../assets/images/blog/IMG_20190619_171157-400.webp" type="image/webp"/>
<source media="(max-width: 399px)" srcset="../../assets/images/blog/IMG_20190619_171157-200.webp" type="image/webp"/>
<img alt="" class="wp-image-1037" height="768" loading="lazy" src="../../assets/images/blog/IMG_20190619_171157.jpg" width="1024"/>
</picture>
</figure></li><li class="blocks-gallery-item"><figure><picture>
<source media="(min-width: 800px)" srcset="../../assets/images/blog/IMG_20190619_172625-800.webp" type="image/webp"/>
<source media="(max-width: 799px)" srcset="../../assets/images/blog/IMG_20190619_172625-400.webp" type="image/webp"/>
<source media="(max-width: 399px)" srcset="../../assets/images/blog/IMG_20190619_172625-200.webp" type="image/webp"/>
<img alt="" class="wp-image-1038" height="768" loading="lazy" src="../../assets/images/blog/IMG_20190619_172625.jpg" width="1024"/>
</picture>
</figure></li></ul>
<p>This day’s honorable mention goes to “The perfect match” paper. Their initial preprocessing of the point cloud reminds me of our <a aria-label="3DmFV (opens in a new tab)" href="https://www.itzikbs.com/what-is-3d-modified-fisher-vector-3dmfv-representation-for-3d-point-clouds" rel="noreferrer noopener" target="_blank">3DmFV</a> representation (though it is very different) and also shows that end-to-end learning is not always the best for point clouds. The day ended with a nice reception with a live band and some state of the … art.</p>
<div class="wp-block-image"><figure class="aligncenter"><picture>
<source srcset="../../assets/images/blog/IMG_20190619_194012-200.webp" type="image/webp"/>
<img alt="" class="wp-image-1025" height="225" loading="lazy" src="../../assets/images/blog/IMG_20190619_194012.jpg" width="300"/>
</picture>
</figure></div>
<h2 class="wp-block-heading">Day 3:</h2>
<p>I presented my poster today! ! ! Therefore I did not have the chance to see most of the posters in the session. However I did get some sneak peaks before the session started. </p>
<ol class="wp-block-list"><li><a href="http://openaccess.thecvf.com/content_CVPR_2019/html/Ben-Shabat_Nesti-Net_Normal_Estimation_for_Unstructured_3D_Point_Clouds_Using_Convolutional_CVPR_2019_paper.html" rel="noreferrer noopener" target="_blank">Nesti-Net: Normal Estimation for Unstructured 3D Point Clouds Using Convolutional Neural Networks</a><br/><a href="http://openaccess.thecvf.com/CVPR2019.py#" rel="noreferrer noopener" target="_blank">Yizhak Ben-Shabat</a>, <a href="http://openaccess.thecvf.com/CVPR2019.py#" rel="noreferrer noopener" target="_blank">Michael Lindenbaum</a>, <a href="http://openaccess.thecvf.com/CVPR2019.py#" rel="noreferrer noopener" target="_blank">Anath Fischer</a><br/>[<a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Ben-Shabat_Nesti-Net_Normal_Estimation_for_Unstructured_3D_Point_Clouds_Using_Convolutional_CVPR_2019_paper.pdf" rel="noreferrer noopener" target="_blank">pdf</a>] [<a href="http://openaccess.thecvf.com/content_CVPR_2019/supplemental/Ben-Shabat_Nesti-Net_Normal_Estimation_CVPR_2019_supplemental.pdf" rel="noreferrer noopener" target="_blank">supp</a>] [<a>bibtex</a>]</li><li><a href="http://openaccess.thecvf.com/content_CVPR_2019/html/Xiang_Generating_3D_Adversarial_Point_Clouds_CVPR_2019_paper.html" rel="noreferrer noopener" target="_blank">Generating 3D Adversarial Point Clouds</a><a href="http://openaccess.thecvf.com/CVPR2019.py#" rel="noreferrer noopener" target="_blank">Chong Xiang</a>, <a href="http://openaccess.thecvf.com/CVPR2019.py#" rel="noreferrer noopener" target="_blank">Charles R. Qi</a>, <a href="http://openaccess.thecvf.com/CVPR2019.py#" rel="noreferrer noopener" target="_blank">Bo Li</a><br/>[<a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Xiang_Generating_3D_Adversarial_Point_Clouds_CVPR_2019_paper.pdf" rel="noreferrer noopener" target="_blank">pdf</a>] [<a href="http://openaccess.thecvf.com/content_CVPR_2019/supplemental/Xiang_Generating_3D_Adversarial_CVPR_2019_supplemental.pdf" rel="noreferrer noopener" target="_blank">supp</a>] [<a>bibtex</a>]</li><li><a href="http://openaccess.thecvf.com/content_CVPR_2019/html/Shi_WarpGAN_Automatic_Caricature_Generation_CVPR_2019_paper.html" rel="noreferrer noopener" target="_blank">WarpGAN: Automatic Caricature Generation</a><br/><a href="http://openaccess.thecvf.com/CVPR2019.py#" rel="noreferrer noopener" target="_blank">Yichun Shi</a>, <a href="http://openaccess.thecvf.com/CVPR2019.py#" rel="noreferrer noopener" target="_blank">Debayan Deb</a>, <a href="http://openaccess.thecvf.com/CVPR2019.py#" rel="noreferrer noopener" target="_blank">Anil K. Jain</a><br/>[<a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Shi_WarpGAN_Automatic_Caricature_Generation_CVPR_2019_paper.pdf" rel="noreferrer noopener" target="_blank">pdf</a>] [<a href="http://openaccess.thecvf.com/content_CVPR_2019/supplemental/Shi_WarpGAN_Automatic_Caricature_CVPR_2019_supplemental.zip" rel="noreferrer noopener" target="_blank">supp</a>] [<a>bibtex</a>]</li><li><a href="http://openaccess.thecvf.com/content_CVPR_2019/html/Chang_Argoverse_3D_Tracking_and_Forecasting_With_Rich_Maps_CVPR_2019_paper.html" rel="noreferrer noopener" target="_blank">Argoverse: 3D Tracking and Forecasting With Rich Maps</a><br/><a href="http://openaccess.thecvf.com/CVPR2019.py#" rel="noreferrer noopener" target="_blank">Ming-Fang Chang</a>, <a href="http://openaccess.thecvf.com/CVPR2019.py#" rel="noreferrer noopener" target="_blank">John Lambert</a>, <a href="http://openaccess.thecvf.com/CVPR2019.py#" rel="noreferrer noopener" target="_blank">Patsorn Sangkloy</a>, <a href="http://openaccess.thecvf.com/CVPR2019.py#" rel="noreferrer noopener" target="_blank">Jagjeet Singh</a>, <a href="http://openaccess.thecvf.com/CVPR2019.py#" rel="noreferrer noopener" target="_blank">Slawomir Bak</a>, <a href="http://openaccess.thecvf.com/CVPR2019.py#" rel="noreferrer noopener" target="_blank">Andrew Hartnett</a>, <a href="http://openaccess.thecvf.com/CVPR2019.py#" rel="noreferrer noopener" target="_blank">De Wang</a>, <a href="http://openaccess.thecvf.com/CVPR2019.py#" rel="noreferrer noopener" target="_blank">Peter Carr</a>, <a href="http://openaccess.thecvf.com/CVPR2019.py#" rel="noreferrer noopener" target="_blank">Simon Lucey</a>, <a href="http://openaccess.thecvf.com/CVPR2019.py#" rel="noreferrer noopener" target="_blank">Deva Ramanan</a>, <a href="http://openaccess.thecvf.com/CVPR2019.py#" rel="noreferrer noopener" target="_blank">James Hays</a><br/>[<a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Chang_Argoverse_3D_Tracking_and_Forecasting_With_Rich_Maps_CVPR_2019_paper.pdf" rel="noreferrer noopener" target="_blank">pdf</a>] [<a href="http://openaccess.thecvf.com/content_CVPR_2019/supplemental/Chang_Argoverse_3D_Tracking_CVPR_2019_supplemental.pdf" rel="noreferrer noopener" target="_blank">supp</a>] [<a>bibtex</a>]</li></ol>
<ul class="wp-block-gallery columns-3 is-cropped wp-block-gallery-3 is-layout-flex wp-block-gallery-is-layout-flex" data-carousel-extra='{"blog_id":1,"permalink":"https:\/\/www.itzikbs.com\/cvpr-2019"}'><li class="blocks-gallery-item"><figure><picture>
<source media="(min-width: 800px)" srcset="../../assets/images/blog/IMG_20190620_082650-800.webp" type="image/webp"/>
<source media="(max-width: 799px)" srcset="../../assets/images/blog/IMG_20190620_082650-400.webp" type="image/webp"/>
<source media="(max-width: 399px)" srcset="../../assets/images/blog/IMG_20190620_082650-200.webp" type="image/webp"/>
<img alt="" class="wp-image-1040" height="768" loading="lazy" src="../../assets/images/blog/IMG_20190620_082650.jpg" width="1024"/>
</picture>
</figure></li><li class="blocks-gallery-item"><figure><picture>
<source media="(min-width: 800px)" srcset="../../assets/images/blog/IMG_20190620_093145-800.webp" type="image/webp"/>
<source media="(max-width: 799px)" srcset="../../assets/images/blog/IMG_20190620_093145-400.webp" type="image/webp"/>
<source media="(max-width: 399px)" srcset="../../assets/images/blog/IMG_20190620_093145-200.webp" type="image/webp"/>
<img alt="" class="wp-image-1041" height="768" loading="lazy" src="../../assets/images/blog/IMG_20190620_093145.jpg" width="1024"/>
</picture>
</figure></li><li class="blocks-gallery-item"><figure><picture>
<source media="(min-width: 800px)" srcset="../../assets/images/blog/IMG_20190620_093239-800.webp" type="image/webp"/>
<source media="(max-width: 799px)" srcset="../../assets/images/blog/IMG_20190620_093239-400.webp" type="image/webp"/>
<source media="(max-width: 399px)" srcset="../../assets/images/blog/IMG_20190620_093239-200.webp" type="image/webp"/>
<img alt="" class="wp-image-1042" height="768" loading="lazy" src="../../assets/images/blog/IMG_20190620_093239.jpg" width="1024"/>
</picture>
</figure></li><li class="blocks-gallery-item"><figure><picture>
<source media="(min-width: 800px)" srcset="../../assets/images/blog/IMG_20190620_160620-800.webp" type="image/webp"/>
<source media="(max-width: 799px)" srcset="../../assets/images/blog/IMG_20190620_160620-400.webp" type="image/webp"/>
<source media="(max-width: 399px)" srcset="../../assets/images/blog/IMG_20190620_160620-200.webp" type="image/webp"/>
<img alt="" class="wp-image-1043" height="768" loading="lazy" src="../../assets/images/blog/IMG_20190620_160620.jpg" width="1024"/>
</picture>
</figure></li></ul>
<h3 class="wp-block-heading">Summary</h3>
<p>It was an overall amazing experience. The sheer magnitude of the event definitely sets some new standards. I came back to Israel highly motivated and full of ideas. I would like to express my appreciation and gratitude to the conference organizers. Small constructive criticism – please reconsider the oral session format (I realize the time constraints are important but the video and unsynced speakers were less than optimal). </p>
<p>It was also a great chance to meet all of my friends and former colleagues from TUM and ANU, and also make some new ones from all around.   <br/>Hope to see you next year! </p>
 

</div>